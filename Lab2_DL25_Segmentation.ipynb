{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW3Lmp7lWomZ"
      },
      "source": [
        "# Lab2: Image Segmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqeTtNgoWsRB"
      },
      "source": [
        "<!--![ObjectDetection](https://drive.google.com/uc?id=1AeG8F2TYEMUWy6huTWBDPsU0NnLr-pyK ). -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JETU9aiJAtAk"
      },
      "source": [
        "![Image Segmentation](https://drive.google.com/uc?id=1TABoIVgRUlUAEaFe_jzG0aOiQM5dk9qX )\n",
        "\n",
        "\n",
        "The segmentation problem in computer vision refers to the division of an image into meaningful regions or segments based on features such as color, intensity, texture, or contours. The primary objective is to distinguish and identify the objects within the image by accurately delineating the boundaries between them.\n",
        "\n",
        "From a computational standpoint, Segmentation can be interpreted as a pixel-level classification problem.\n",
        "\n",
        "Segmentation provides fine-grained information about object boundaries and regions, while detection focuses on identifying specific objects and their locations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1dnfZRHU4w3"
      },
      "source": [
        "**Some uses cases**\n",
        "\n",
        "\n",
        "*   Autonomous Vehicles\n",
        "*   Medical Imaging Analysis\n",
        "*   Analysis of Satellite Images\n",
        "*   Smart Agriculture\n",
        "*   Industrial Inspection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below the input and output of a classic simple segmentation problem."
      ],
      "metadata": {
        "id": "taAGkj0CL-Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio.v2 as imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "img = imageio.imread(\"https://drive.google.com/uc?id=1oMwhUwK-tuCaAP-_2aiTyoCrLIcqkDc9\")\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "print(f\"Input dim: {img.shape}\")  # HWC\n",
        "\n",
        "mask = imageio.imread(\"https://drive.google.com/uc?id=1gG8woOuLu2X4dHotXY5GKSZbHvMeOOoo\")\n",
        "plt.imshow(mask)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "print(f\"Output dim: {mask.shape}\")  # HWC\n",
        "\n"
      ],
      "metadata": {
        "id": "mdKH1FahL87O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgYiEZeUn9aE"
      },
      "source": [
        "### What is a mask?\n",
        "\n",
        "To highlight specific parts of a tensor containing our data, it is possible to create another tensor of the same dimensions, where each element corresponding to our target takes the value 1 (True) or 0 (False). This tensor is called a \"mask\" and acts as a filter, allowing the model to \"turn on\" or \"turn off\" the regions.\n",
        "\n",
        "In Deep Learning, a mask can take on different meanings depending on its usage, but the core concept remains the same.\n",
        "\n",
        "\n",
        "Key Applications of Masking\n",
        "1. Ground Truth Masks in Semantic Segmentation or Object Detection.\n",
        "2. Transformer Attention Mechanisms to control\n",
        "3. Data Augmentation: Random Erasing/Occlusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How mask filter ?"
      ],
      "metadata": {
        "id": "6xXnkEnQRyOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print( np.unique(mask) )\n",
        "mask//=255\n",
        "print( np.unique(mask) )\n",
        "mask = mask[...,None]\n",
        "print( img.shape)\n",
        "print( mask.shape)"
      ],
      "metadata": {
        "id": "MqF97fxQTCDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMlrebsWadtp"
      },
      "outputs": [],
      "source": [
        "new_img = img*mask\n",
        "plt.imshow(new_img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "print(f\"Output dim: {new_img.shape}\")  # HWC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJRAQaEMpUKB"
      },
      "source": [
        "#Image Segmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uyiEMrPp60gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we want to train a model to generate a mask from unseen samples ?"
      ],
      "metadata": {
        "id": "ssYbsLWUYee8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQflTasopq7C"
      },
      "source": [
        "\n",
        "This is generally done with a Convolutional Neural Network that act as an image-to-image transform, mapping each pixel of $x$ to the corresponding class.\n",
        "\n",
        "*Remind:* Given an image $x \\in \\mathbb{R}^{m \\times n \\times c}$, an image-to-image map is a function $f: \\mathbb{R}^{m \\times n \\times c} \\to \\mathbb{R}^{m' \\times n' \\times c'}$. In our situation, $m = m'$, $n = n'$ and $c = c'$. An image-to-image map is required to do segmentation and some image processing tasks, but not for classification or object detection. \\\\\n",
        "\n",
        "Image-to-image maps are usually implemented by some variant of a Fully Convolutional Neural Network (FNN) design (e.g. ResNet, Autoencoders, ...). See https://heartbeat.comet.ml/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abo48tFBqDQ5"
      },
      "source": [
        "![Image Segmentation](https://drive.google.com/uc?id=1A2zksYq9ehq5ghNKl2sOZfkochbbgFDG )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k3KsJA5pqG5"
      },
      "source": [
        "**Semantic segmentation** separates different classes at the pixel level but does not separate different instances of the same class. The **Object detection** separates instances but provides only a crude approximation of the instance shape (the box).The task of **Instance segmentation** lays at the intersection of the two. It can be defined as the task of detecting all instances of the objects of interest in an image and classifying them; and segmenting them from the background at the pixel level.\n",
        "\n",
        "![Image Segmentation](https://drive.google.com/uc?id=1xO189Tlv8GbE86sncajEn6LmkE3NTu6l )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-FGBAXn2m8I"
      },
      "source": [
        "###Some Segmentation datasets:\n",
        "\n",
        "*   Pascal2:11540 images,6,929 segmentation masks and 20 categories.\n",
        "     http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html:\n",
        "\n",
        "*   ADEK20:images 25K+2K and 150 categories.\n",
        "     https://groups.csail.mit.edu/vision/datasets/ADE20K/\n",
        "*   Cityscapes Dataset: https://www.cityscapes-dataset.com/\n",
        "*   Mapillar Vistas: https://www.mapillary.com/dataset/vistas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUF0M09rrBv5"
      },
      "outputs": [],
      "source": [
        "!pip install keras_cv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J488Yr2xtRoF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "from pathlib import Path\n",
        "import keras\n",
        "import keras_cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Image Config\n",
        "HEIGHT = 64\n",
        "WIDTH = 64\n",
        "NUM_CLASSES = 3\n",
        "AUTOTUNE = tf.data.AUTOTUNE #optimize hw performance automatically\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "dir=\"./model_weights\"\n",
        "if not Path(dir).exists():\n",
        "  os.mkdir(dir)\n",
        "  print(\"Folder was created\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kw2dC99BqeIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2e5RIkwYkZj"
      },
      "source": [
        "In this tutorial we are going to introduce [The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMBUqV6_t5o-"
      },
      "outputs": [],
      "source": [
        "#Download the datasets\n",
        "tfds.disable_progress_bar()\n",
        "orig_train_ds, orig_val_ds = tfds.load( name=\"oxford_iiit_pet\", split=[\"train+test[:80%]\", \"test[80%:]\"] )\n",
        "\n",
        "#{\n",
        "#  'image': tf.Tensor,               # immagine RGB del pet\n",
        "#  'label': tf.Tensor,               # classe (razza del pet)\n",
        "#  'segmentation_mask': tf.Tensor,  # maschera di segmentazione (pixel-wise)\n",
        "#}\n",
        "#Le maschere (segmentation_mask) sono incluse automaticamente nel dataset da tfds\n",
        "print(orig_val_ds.element_spec)\n",
        "#orig_train_ds = contiene il training + 80% del test\n",
        "#Questo tipo di split è utile se vuoi fare una nuova divisione del dataset, ad esempio per addestrare un modello con più dati o valutare su un subset specifico.\n",
        "#Calcola il numero di elementi in ciascun dataset\n",
        "tf.data.experimental\n",
        "num_train_elements = tf.data.experimental.cardinality(orig_train_ds).numpy()\n",
        "num_val_elements = tf.data.experimental.cardinality(orig_val_ds).numpy()\n",
        "\n",
        "print(\"Training set length:\", num_train_elements)\n",
        "print(\"Test set length:\", num_val_elements)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4mFeA3fuUfc"
      },
      "outputs": [],
      "source": [
        "#data preprocessing\n",
        "def rescale_images_and_correct_masks( inputs):\n",
        "  return {\n",
        "    \"images\": tf.cast(inputs[\"image\"], dtype=tf.float32) / 255.0, #normalization\n",
        "    \"segmentation_masks\": inputs[\"segmentation_mask\"] - 1, #put all values as 0-based.\n",
        "  }\n",
        "#now the label of the ground truth pixels are 0 for pet, 1 for borders, 2 for background\n",
        "\n",
        "\n",
        "#utility function, Serve a convertire il dizionario {images, segmentation_masks} in una tupla,come richiesto da Keras nei modelli supervisati.\n",
        "def unpackage_inputs(inputs):\n",
        "    images = inputs[\"images\"]\n",
        "    segmentation_masks = inputs[\"segmentation_masks\"]\n",
        "    return images, segmentation_masks\n",
        "\n",
        "\n",
        "train_ds = orig_train_ds.map( rescale_images_and_correct_masks, num_parallel_calls=AUTOTUNE )\n",
        "val_ds = orig_val_ds.map(rescale_images_and_correct_masks, num_parallel_calls=AUTOTUNE)\n",
        "#map applica una funzione a ogni elemento del dataset, uno per uno (o in parallelo),senza usare un ciclo for esplicito.\n",
        "#Un thread è una unità di esecuzione all'interno di un processo,AUTOTUNE:TensorFlow sceglie automaticamente il numero ottimale di thread\n",
        "\n",
        "\n",
        "resize_fn = keras_cv.layers.Resizing( HEIGHT, WIDTH ) #ridimensiona le immagini e le maschere a dimensioni fisse\n",
        "\n",
        "resized_val_ds = (\n",
        "    val_ds.map(resize_fn, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(unpackage_inputs)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE) #Dataset finale pronto per il training\n",
        ")\n",
        "\n",
        "resized_train_ds = (\n",
        "    train_ds.map(resize_fn, num_parallel_calls=AUTOTUNE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(unpackage_inputs)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE) # carica i batch in anticipo, per migliorare le prestazioni durante il training\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3zalPSAuiLp"
      },
      "outputs": [],
      "source": [
        "plot_train_ds = train_ds.map(unpackage_inputs).ragged_batch(9)#4  #ragged handles variable image shapes\n",
        "\n",
        "images, segmentation_masks = next(iter(plot_train_ds.take(1)))\n",
        "\n",
        "print( f\"Image Shape: {images.shape}\"  )\n",
        "print( f\"Segmentation Mask Shape: {segmentation_masks.shape}\"  )\n",
        "\n",
        "keras_cv.visualization.plot_segmentation_mask_gallery(\n",
        "    images,\n",
        "    value_range=(0,1),#-1\n",
        "    num_classes=3,\n",
        "    y_true=segmentation_masks,\n",
        "    y_pred=None,\n",
        "    scale=3,\n",
        "    rows=3,#2\n",
        "    cols=3,#2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "qMSgTsjHvM8N"
      },
      "outputs": [],
      "source": [
        "#Create a callback\n",
        "\n",
        "# Taking a batch of test inputs to measure model's progress.\n",
        "test_images, test_masks = next(iter(resized_val_ds))\n",
        "\n",
        "class DisplayCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, epoch_interval=None):\n",
        "        self.epoch_interval = epoch_interval\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.epoch_interval and epoch % self.epoch_interval == 0:  #Viene eseguito alla fine di ogni epoca.\n",
        "\n",
        "            self.model.save_weights(save_path)\n",
        "\n",
        "            pred_masks = self.model.predict(test_images)\n",
        "            pred_masks = tf.math.argmax(pred_masks, axis=-1)\n",
        "            pred_masks = pred_masks[..., tf.newaxis] #add a new dimension at the end of pred_masks.\n",
        "            # ... is a placeholder for dimensions\n",
        "\n",
        "            # Randomly select an image from the test batch\n",
        "            random_index = random.randint(0, BATCH_SIZE - 1)\n",
        "            random_image = test_images[random_index]\n",
        "            random_pred_mask = pred_masks[random_index]\n",
        "            random_true_mask = test_masks[random_index]\n",
        "\n",
        "            fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n",
        "            ax[0].imshow(random_image)\n",
        "            ax[0].set_title(f\"Image: {epoch:03d}\")\n",
        "\n",
        "            ax[1].imshow(random_true_mask)\n",
        "            ax[1].set_title(f\"Ground Truth Mask: {epoch:03d}\")\n",
        "\n",
        "            ax[2].imshow(random_pred_mask)\n",
        "            ax[2].set_title(f\"Predicted Mask: {epoch:03d}\", )\n",
        "\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\",\n",
        "    restore_best_weights=True,\n",
        "    start_from_epoch=0,\n",
        "    patience=3\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = [DisplayCallback(5),early_stopping]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0H9EcFHd2NW"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLhj7r6tv5q6"
      },
      "source": [
        "## UNet\n",
        "\n",
        "Maybe the most known network architecture used for segmentation is the UNet.\n",
        "\n",
        "![](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n",
        "\n",
        "The basics are:\n",
        "\n",
        "  - On downsampling (MaxPooling2D layer), the dimensions in Height and Width is divided by 2 while Filters Channels are dubled\n",
        "\n",
        "  - On upsampling (ConvTranspose2D layer), Height and Width are doubled and Filter channels halved.\n",
        "\n",
        "  - 2 layers of Conv2D (3x3, padding=same) for each block\n",
        "\n",
        "  - The bottleneck captures global context information\n",
        "\n",
        "  - Skipping connections help decoder to generate a map.\n",
        "  \n",
        "  - Final layer is typically a 1×1 convolution that maps the feature vector to the desired number of classes. (1 for us)\n",
        "  \n",
        "\n",
        "This factors and others leads to strong training stability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZzheTANvYVh"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
        "\n",
        "def down_block(inputs, filters, kernel_size=(3, 3), padding='same', activation='relu'):\n",
        "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(inputs)\n",
        "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(conv)\n",
        "    pool = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "    return conv, pool\n",
        "\n",
        "def up_block(inputs, skip, filters, kernel_size=(3, 3), padding='same', activation='relu'):\n",
        "    up = UpSampling2D(size=(2, 2))(inputs)\n",
        "    concat = Concatenate()([up, skip])\n",
        "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(concat)\n",
        "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(conv)\n",
        "    return conv\n",
        "\n",
        "\n",
        "def unet(img_size=(256, 256, 1), num_classes=1):\n",
        "    inputs = Input(shape=img_size + (3,))\n",
        "\n",
        "    #Down Blocks\n",
        "    conv1, pool1 = down_block(inputs, 64)\n",
        "    conv2, pool2 = down_block(pool1, 128)\n",
        "    conv3, pool3 = down_block(pool2, 256)\n",
        "\n",
        "    # Bottleneck\n",
        "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
        "\n",
        "    #Up Blocks\n",
        "    conv5 = up_block(conv4, conv3, 256)\n",
        "    conv6 = up_block(conv5, conv2, 128)\n",
        "    conv7 = up_block(conv6, conv1, 64)\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = keras.layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(conv7)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name=\"unet\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# Build model\n",
        "model = unet(img_size=(HEIGHT, WIDTH), num_classes=NUM_CLASSES)\n",
        "#save_path=f\"./model_weights/weights_img{HEIGHT}x{WIDTH}.weights.h5\"\n",
        "save_path=f\"/content/drive/MyDrive/IAALAB_2324/Lezione5/weights_img{HEIGHT}x{WIDTH}.weights.h5\"\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "    print('TensorFlow is using GPU/TPU')\n",
        "else:\n",
        "    print('TensorFlow is using CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-5pjldNvc_z"
      },
      "outputs": [],
      "source": [
        "# Training Config\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(LEARNING_RATE),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_weights(save_path)\n",
        "    print(\"The model was loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"No weights loaded:\\n{e}\")\n",
        "\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch.\n",
        "history = model.fit(\n",
        "    resized_train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=resized_val_ds,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPx5xjmYvgo2"
      },
      "outputs": [],
      "source": [
        "model.save_weights(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8bAMlgBqCTm"
      },
      "outputs": [],
      "source": [
        "# np.save(\"val_accuracy\", hist.history['val_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsDRwGEFvhfy"
      },
      "outputs": [],
      "source": [
        "model.load_weights(save_path)\n",
        "\n",
        "pred_masks = model.predict(test_images)\n",
        "pred_masks = tf.math.argmax(pred_masks, axis=-1)[..., None]\n",
        "\n",
        "keras_cv.visualization.plot_segmentation_mask_gallery(\n",
        "    test_images,\n",
        "    value_range=(0, 1),\n",
        "    num_classes=3,\n",
        "    y_true=test_masks,\n",
        "    y_pred=pred_masks,\n",
        "    scale=4,\n",
        "    rows=2,\n",
        "    cols=4,\n",
        ")\n",
        "\n",
        "#original source code at https://keras.io/examples/vision/oxford_pets_image_segmentation/#prediction-with-trained-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KG8KIVTvuGC"
      },
      "outputs": [],
      "source": [
        "# Check overfit\n",
        "loss_history = history.history['loss']\n",
        "val_loss_history = history.history['val_loss']\n",
        "\n",
        "acc_history = history.history['accuracy']\n",
        "val_acc_history = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.plot(val_loss_history)\n",
        "plt.grid()\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Loss', 'Val Loss'])\n",
        "plt.title('Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(acc_history)\n",
        "plt.plot(val_acc_history)\n",
        "plt.grid()\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Accuracy', 'Val Accuracy'])\n",
        "plt.title('Accuracy')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbGZf-7y-pAs"
      },
      "source": [
        "The Intersection over Union (IoU) is a metrics used in segmentation problems.\n",
        "\n",
        " It is defined as the ratio between the intersection area between the predicted mask and the ground truth mask, over the union between the two masks.\n",
        "\n",
        "![](https://miro.medium.com/max/300/0*kraYHnYpoJOhaMzq.png)\n",
        "\n",
        "\n",
        "*   IoU scores close to 1.0 indicate a high level of accuracy. The predicted and ground truth areas overlap perfectly, meaning the area of overlap equals the area of union.\n",
        "\n",
        "when the predicted and groud truth areas overlap eachother perfectly. In other words, if the area of overlap is the same of the area of union.\n",
        "\n",
        "*   Conversely, IoU scores close to 0 suggest poor accuracy, indicating little to no overlap between the predicted and ground truth regions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bze7NbzvExZq"
      },
      "source": [
        "In the case of segmentation, the intersection over union is computed considering each individual pixel in the predicted segmentation mask and the ground truth mask.\n",
        "\n",
        "$$\n",
        "IoU_{pixel} = \\frac{\\sum_{i=1}^{N}(P_i \\cap G_i)}{\\sum_{i=1}^{N}(P_i \\cup G_i)}\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "*   *Pi* is the predicted segmentation mask at pixel i,\n",
        "*   *Gi* is the ground truth mask at pixel i,\n",
        "*   *∩* denotes the intersection operator,\n",
        "*   *∪* denotes the union operator, and\n",
        "*   *N* is the total number of pixels in the image.\n",
        "\n",
        "**Recap of accuracy formula:**\n",
        "$$\n",
        "Accuracy = \\frac{Number\\ of\\ right\\ predictions}{Total\\ number\\ of\\ predictions}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfZImd7mNhjq"
      },
      "outputs": [],
      "source": [
        "# import keras.ops as K\n",
        "import keras.backend as K\n",
        "\n",
        "def iou_coeff(y_true, y_pred):\n",
        "    print(\"y_true shape\", y_true.shape)\n",
        "    print(\"y_pred shape\", y_pred.shape)\n",
        "    smooth = 1\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
        "    union = K.sum(y_true,[1,2,3]) + K.sum(y_pred,[1,2,3]) - intersection\n",
        "    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
        "    return iou\n",
        "\n",
        "def evaluate_model(model, validation_dataset, fun):\n",
        "    y_pred = model.predict(validation_dataset)\n",
        "    y_true = np.concatenate([y for x, y in validation_dataset], axis=0)\n",
        "    y_pred = y_pred.astype('float32')\n",
        "    y_true = y_true.astype('float32')\n",
        "    evaluation_result = fun(y_true, y_pred)\n",
        "    return evaluation_result\n",
        "\n",
        "iou = evaluate_model(model, resized_val_ds, iou_coeff)\n",
        "print(\"Pixel wise IoU:\", iou.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAxWAZffNL_a"
      },
      "source": [
        "Try to experiment by yourself at home."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zceArwg8HgYo"
      },
      "source": [
        "Some suggested readings:\n",
        "\n",
        "*  [Panoptic Segmentation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.pdf)\n",
        "\n",
        "*    [Panoptic FeaturePyramidNetwork](https://arxiv.org/pdf/1901.02446.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhXgnGzH-aCM"
      },
      "source": [
        "###Intersection over union"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAM: Segment anything model\n",
        "\n",
        "from Meta AI lab research\n",
        "\n"
      ],
      "metadata": {
        "id": "qX3tvXiebVLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YRqVBJ0o0SMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio.v2 as imageio\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "import imageio.v2 as imageio\n",
        "\n",
        "\n",
        "def print_images(imgs):\n",
        "  fig, axes = plt.subplots(1, len(imgs), figsize=(4 * len(imgs), 4))\n",
        "  axes = [axes] if len(imgs) == 1 else axes #make ax iterable: ax != ax[0]\n",
        "  for i, img in enumerate(imgs):\n",
        "      axes[i].imshow(img)\n",
        "      axes[i].set_title(f\"shape: {img.shape}\")\n",
        "      axes[i].axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "urls = [\n",
        "    \"https://drive.google.com/uc?id=1vWgkE4_TUuBJhlsZv7JdURRCM_WoYNRl\",\n",
        "    \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\" #is RGBA\n",
        "]\n",
        "\n",
        "imgs = [imageio.imread(url)[..., :3] for url in urls]\n",
        "\n",
        "print_images(imgs)\n"
      ],
      "metadata": {
        "id": "pnAvCWaIaZVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate random points to prompt the model"
      ],
      "metadata": {
        "id": "JYwXcvA17FXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_points(img_size, num_points=250, distance_factor=0.5):\n",
        "    height, width = img_size\n",
        "\n",
        "    # The distance from top-left corner of the image decrease with num of points\n",
        "    topl_distance_pct = max(0.05, 1 / np.sqrt(num_points))\n",
        "    #side is the distance from ..\n",
        "    side = min(width, height) * distance_factor\n",
        "\n",
        "    #Compute randomly the point on the top left corner\n",
        "    x0 = np.random.uniform( width*topl_distance_pct, width - side)\n",
        "    y0 = np.random.uniform( height*topl_distance_pct, height - side)\n",
        "\n",
        "    # Generate uniformely distributed points\n",
        "    points = []\n",
        "\n",
        "\n",
        "    for _ in range(num_points):\n",
        "        x = np.random.uniform(x0, x0 + side)\n",
        "        y = np.random.uniform(y0, y0 + side)\n",
        "        points.append([x, y])\n",
        "\n",
        "    return points\n",
        "\n",
        "num_points=4\n",
        "# we generate random points\n",
        "pnts=[\n",
        "    sample_points(imgs[0].shape[:2],num_points ),\n",
        "    sample_points(imgs[1].shape[:2],num_points )\n",
        "]\n",
        "\n",
        "for p in pnts:\n",
        "  print(p)\n",
        "\n",
        "def add_redpoints_toimg_fun(im, pts):\n",
        "  itmp = im.copy()\n",
        "  min_len = min(im.shape[:2])\n",
        "  print(im.shape)\n",
        "  print(len(pts))\n",
        "\n",
        "  for pt in pts:\n",
        "    x, y = int(pt[0]), int(pt[1])\n",
        "    blue=(0, 0, 255) #rgb\n",
        "    rds = min_len//50\n",
        "    cv2.circle(itmp, (x, y), color=blue, radius=rds, thickness=-1)\n",
        "  return itmp\n",
        "\n",
        "\n",
        "modified_ims = [ add_redpoints_toimg_fun(im,pts) for im, pts in zip(imgs, pnts) ]\n",
        "\n",
        "#show output\n",
        "print_images(modified_ims)"
      ],
      "metadata": {
        "id": "HhlhA64K7DKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFSamModel, SamProcessor\n",
        "\n",
        "# Load the model and processor\n",
        "model = TFSamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
        "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
        "\n",
        "preprocessing_fun = processor\n",
        "postprocessing_fun = processor.image_processor.post_process_masks"
      ],
      "metadata": {
        "id": "FQrvMBtFaknb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SAM inference\n",
        "\n",
        "#imgs : the list of images\n",
        "#pnts : the list of points\n",
        "inputs = preprocessing_fun( images=imgs,  input_points=pnts, return_tensors=\"tf\",multimask_output=False)\n",
        "\n",
        "outputs = model(inputs)\n",
        "\n",
        "preds = outputs.pred_masks\n",
        "scores = outputs.iou_scores\n",
        "\n",
        "masks = postprocessing_fun(\n",
        "    masks=preds,\n",
        "    original_sizes=inputs[\"original_sizes\"],\n",
        "    reshaped_input_sizes=inputs[\"reshaped_input_sizes\"],\n",
        "    return_tensors=\"tf\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "GBVk97lLpEB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in masks:\n",
        "  print(m.shape)\n",
        "\n",
        "reshaped_masks = [ m.numpy().squeeze().transpose(1,2,0).astype(np.float32) for m in masks ]\n",
        "\n",
        "print( reshaped_masks[0].shape )\n",
        "print( reshaped_masks[1].shape )\n",
        "\n",
        "#reshaped = np.array()..astype(np.float32) #from CHW to HWC\n",
        "print_images(reshaped_masks)"
      ],
      "metadata": {
        "id": "lAhBtuhhNkie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_grid(img_size, num_points):\n",
        "    h,w = img_size\n",
        "    num_points_x = int(np.sqrt(num_points * (w / h)))  # Colonne\n",
        "    num_points_y = num_points // num_points_x  # Righe\n",
        "\n",
        "    # Calcola la distanza tra i punti\n",
        "    distance_x = w / num_points_x\n",
        "    distance_y = h / num_points_y\n",
        "\n",
        "    # Crea la griglia\n",
        "    return [[x * distance_x, y * distance_y] for x in range(num_points_x) for y in range(num_points_y)]\n",
        "\n",
        "def generate_grid(img_size, rows, cols):\n",
        "    h, w = img_size\n",
        "\n",
        "    # Calcola la spaziatura\n",
        "    step_x = w / cols\n",
        "    step_y = h / rows\n",
        "\n",
        "    # Genera la griglia\n",
        "    grid = []\n",
        "    for row in range(rows):\n",
        "        for col in range(cols):\n",
        "            x = col * step_x + step_x / 2  # Centra il punto nella cella\n",
        "            y = row * step_y + step_y / 2  # Centra il punto nella cella\n",
        "            grid.append([x, y])\n",
        "\n",
        "    return grid\n",
        "\n",
        "rows_points=3\n",
        "cols_points=3\n",
        "\n",
        "pnts=[\n",
        "    generate_grid(imgs[0].shape[:2], rows_points, cols_points),\n",
        "    generate_grid(imgs[1].shape[:2], rows_points, cols_points)\n",
        "]\n",
        "\n",
        "modified_ims = [ add_redpoints_toimg_fun(im,pts) for im, pts in zip(imgs, pnts) ]\n",
        "#show output\n",
        "print_images(modified_ims)\n",
        "\n"
      ],
      "metadata": {
        "id": "tBkx5FQ7dr7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the experiment"
      ],
      "metadata": {
        "id": "yOAOSFmEmG8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SAM inference\n",
        "\n",
        "#imgs : the list of images\n",
        "#pnts : the list of points\n",
        "inputs = preprocessing_fun( images=imgs,  input_points=pnts, return_tensors=\"tf\",multimask_output=False)\n",
        "\n",
        "outputs = model(inputs)\n",
        "\n",
        "preds = outputs.pred_masks\n",
        "scores = outputs.iou_scores\n",
        "\n",
        "masks = postprocessing_fun(\n",
        "    masks=preds,\n",
        "    original_sizes=inputs[\"original_sizes\"],\n",
        "    reshaped_input_sizes=inputs[\"reshaped_input_sizes\"],\n",
        "    return_tensors=\"tf\"\n",
        ")\n",
        "\n",
        "for m in masks:\n",
        "  print(m.shape)\n",
        "\n",
        "reshaped_masks = [ m.numpy().squeeze().transpose(1,2,0).astype(np.float32) for m in masks ]\n",
        "\n",
        "print( reshaped_masks[0].shape )\n",
        "print( reshaped_masks[1].shape )\n",
        "\n",
        "#reshaped = np.array()..astype(np.float32) #from CHW to HWC\n",
        "print_images(reshaped_masks)"
      ],
      "metadata": {
        "id": "Ino2p_ZdmAkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a30S1VwEouAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}